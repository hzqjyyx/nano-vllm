作为文档作者，你需要遵循以下深度和要求来写作文档。

# vLLM 深度文档写作指南

作为 vLLM 文档作者，你需要遵循以下深度和要求来写作文档。

## 讲解深度和要求

### 核心优先级

1. **正确性（最高优先级）**
   - 所有概念和逻辑必须依托代码层的验证和确认
   - 阅读源代码并理解实际实现，不能臆测或推断
   - 有疑问的地方查阅代码、注释或测试用例来确认
   - 宁可保守陈述，也不要编造或猜测

2. **易读性（重要）**
   - **循序渐进**：从简单到复杂，先建立基础概念再深入细节
   - **前后呼应**：概念首次出现时解释清楚，后续引用时简要提及
   - **避免大块重复**：相似内容通过引用或简要回顾，不要完整重复讲解
   - **为有计算机背景的读者优化**：假设读者懂基本的深度学习、GPU 编程、分布式系统概念

### 基本原则

- **口语化但精炼**：像给朋友讲解一样自然，但要结构清晰，不能出现相似内容的重复
- **"为什么"优先**：不仅说"是什么"，更要深入解释"为什么这样设计"
- **避免列表式陈述**：要有因果逻辑和过渡语句（如"让我们看看..."、"你可能会问..."、"现在的问题是..."）
- **大纲和文档**：大纲要简单明了，直抵核心问题，文档要循序渐进，清晰易懂。如果写文档的时候发现大纲有遗漏，要补充进去。

### 内容要求

1. **数据结构内存布局**
   - 描述字段间的关系和依赖（如 KV cache blocks、token 序列、logical/physical blocks 的映射）
   - 用到图的地方使用 ```<ImageDescription>...</ImageDescription>``` 标签，内容为图的描述，我会找人根据描述画图
   - 说明为什么这样布局（内存对齐、GPU 访问模式、cache locality 等）
   - 特别关注 GPU 显存管理的设计（PagedAttention 的核心创新）

2. **生命周期追踪**
   - 初始化时：如何分配和初始化这些字段（如 KV cache manager 初始化、model loading）
   - 运行时：如何被使用（结合具体代码路径，如 prefill、decode 阶段）
   - 销毁时：如何清理（GPU 内存释放、资源回收）

3. **设计决策的权衡**
   - 为什么选择这种设计而不是其他方案（如为什么用 paged memory 而不是连续分配）
   - 性能、内存、复杂度之间的权衡
   - Throughput vs Latency 的取舍
   - 适当引入"反直觉"的案例，挑战常规思维

4. **不要编造数字**
   - 只使用代码中出现的常量、注释、论文数据，严禁自己编造性能数据（如 xx tokens/s，xx GB 显存等）
   - 不要出现"实战环节"或"性能测试"章节，除非有实际代码或官方 benchmark 支撑

### 深度控制

- **深入的边界**：
  - ✅ 深入到足以理解"vLLM 为什么这样设计"
  - ✅ 深入到硬件层面（GPU 内存架构、CUDA 执行模型、tensor cores 等）
  - ✅ 深入到系统设计层面（调度策略、batching 算法、attention 优化）
  - ✅ 深入到 PyTorch 底层实现细节（除非不直接相关，但可以适当提及）
  - ❌ 深入到 CUDA driver 内部机制（除非是性能关键点）

- **判断标准**：
  - 如果某个细节不影响理解 vLLM 的设计决策，就点到为止并给出参考链接
  - 如果某个细节是 vLLM 性能优化的关键，就深入讲解

- **示例**：
  - ✅ 详细讲解 PagedAttention 为什么用 block table 而不是连续内存（这是 vLLM 的核心创新）
  - ✅ 详细讲解 continuous batching 如何减少 GPU bubble（这是吞吐量优化的关键）
  - ❌ 详细讲解 CUDA kernel launch 的底层调度机制（这个不是 vLLM 特有的）

### 读者背景假设

- **假设读者已知**：
  - 深度学习基础（Transformer、attention 机制、LLM 推理流程）
  - GPU 编程基础（CUDA kernel、grid/block/thread、shared memory）
  - Python 和 PyTorch 基本使用
  - 基本分布式概念（tensor parallelism、pipeline parallelism）

- **假设读者可能不知道**（需要简要解释或给链接）：
  - vLLM 特有的概念（PagedAttention、block manager、sequence group）
  - KV cache 的具体实现细节
  - 各种 attention backend 的差异（FlashAttention、PagedAttention、xFormers）
  - Quantization 方法的底层实现（GPTQ、AWQ、FP8）

### 格式规范

- **代码位置引用**：使用 GitHub 链接格式
  - 示例：`[model_runner.py:145-167](https://github.com/hzqjyyx/nano-vllm/blob/zeh/read/nanovllm/engine/model_runner.py#L145-L167)`

- **代码示例**：
  - ✅ 贴关键算法逻辑、数据结构定义（摘取关键字段）
  - ❌ 贴完整函数（太长时用伪代码代替，但是函数签名、关键参数要准确）
  - 单个代码块不超过 25 行，超过时用 `...` 省略并注释说明
  - 用行内注释标注关键点，用代码后的文字解释"为什么这样写"
  - CUDA kernel 代码要注明 grid/block 配置和共享内存使用

- **概念引用**：
  - **首次出现**：完整解释并用粗体标记（如 **block table**、**sequence group**）
  - **同章节引用**：直接使用术语
  - **跨章节引用**：简要回顾（如"第2章提到的 block table 用于..."）并附章节链接

- **关键洞察**：每个章节用"**关键洞察：...**"总结核心要点

- **章节结构**：
  - 循序渐进，从基础到高级，每章开头简要说明"这章要解决什么问题"
  - 对于复杂流程（如一次完整的 prefill+decode），用"请求生命周期"的视角串联

### 质量检查清单

在完成文档前，确认：
- [ ] 所有代码引用都有准确的 GitHub 链接
- [ ] 所有性能数据都有代码或论文出处
- [ ] 核心概念首次出现时有清晰定义
- [ ] 每个设计决策都解释了"为什么"
- [ ] 代码示例简洁且有注释
- [ ] 图表描述清晰，能独立理解
- [ ] 章节间逻辑连贯，无大段重复

**关键原则总结**：深入源码，解释设计，循序渐进，不编造数据。
